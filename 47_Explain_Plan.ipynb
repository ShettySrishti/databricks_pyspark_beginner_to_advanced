{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beaff282-8331-44e9-af77-5ba865b77a0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Srishti</td><td>Shetty</td><td>1998</td><td>100</td><td>F</td><td>8000</td></tr><tr><td>20</td><td>Aish</td><td>Rai</td><td>2002</td><td>200</td><td>M</td><td>2000</td></tr><tr><td>30</td><td>Rishabh</td><td>Alva</td><td>2010</td><td>100</td><td>null</td><td>6000</td></tr><tr><td>40</td><td>Aarvi</td><td>Bhandhary</td><td>1996</td><td>400</td><td>F</td><td>7000</td></tr><tr><td>50</td><td>Pooja</td><td>Hegde</td><td>2008</td><td>500</td><td>F</td><td>5000</td></tr><tr><td>60</td><td>Sunil</td><td>Shetty</td><td>1997</td><td>400</td><td>M</td><td>3000</td></tr><tr><td>70</td><td>Sid</td><td>Rai</td><td>2010</td><td>600</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Srishti",
         "Shetty",
         "1998",
         "100",
         "F",
         8000
        ],
        [
         20,
         "Aish",
         "Rai",
         "2002",
         "200",
         "M",
         2000
        ],
        [
         30,
         "Rishabh",
         "Alva",
         "2010",
         "100",
         null,
         6000
        ],
        [
         40,
         "Aarvi",
         "Bhandhary",
         "1996",
         "400",
         "F",
         7000
        ],
        [
         50,
         "Pooja",
         "Hegde",
         "2008",
         "500",
         "F",
         5000
        ],
        [
         60,
         "Sunil",
         "Shetty",
         "1997",
         "400",
         "M",
         3000
        ],
        [
         70,
         "Sid",
         "Rai",
         "2010",
         "600",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "first_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "employee_data = [(10, \"Srishti\",\"Shetty\", \"1998\", \"100\", \"F\", 8000),\n",
    "                 (20, \"Aish\", \"Rai\", \"2002\", \"200\", \"M\", 2000),\n",
    "                 (30, \"Rishabh\", \"Alva\", \"2010\", \"100\",None, 6000),\n",
    "                 (40, \"Aarvi\",  \"Bhandhary\", \"1996\", \"400\", \"F\", 7000),\n",
    "                 (50, \"Pooja\",  \"Hegde\", \"2008\", \"500\", \"F\", 5000),\n",
    "                 (60, \"Sunil\", \"Shetty\", \"1997\", \"400\", \"M\", 3000),\n",
    "                 (70, \"Sid\", \"Rai\", \"2010\", \"600\", \"M\", 5000)\n",
    "                 ]\n",
    "\n",
    "employee_schema = [\"employee_id\", \"first_name\", \"last_name\", \"doj\", \"employee_dept_id\", \"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data = employee_data, schema = employee_schema)\n",
    "display(empDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe2d4af-80b0-4030-abde-fa2ec7485c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dept_name</th><th>dept_id</th></tr></thead><tbody><tr><td>HR</td><td>100</td></tr><tr><td>Supply</td><td>200</td></tr><tr><td>Sales</td><td>300</td></tr><tr><td>Stock</td><td>400</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "HR",
         100
        ],
        [
         "Supply",
         200
        ],
        [
         "Sales",
         300
        ],
        [
         "Stock",
         400
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dept_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept_id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "department_data = [(\"HR\", 100),\n",
    "                   (\"Supply\", 200),\n",
    "                   (\"Sales\", 300),\n",
    "                   (\"Stock\", 400)]\n",
    "\n",
    "department_schema = [\"dept_name\", \"dept_id\"]\n",
    "deptDF= spark.createDataFrame(department_data, department_schema)\n",
    "display(deptDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c7d93e0-0d3d-4b34-aa92-2a020ef0fe77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99a7b675-71c8-4586-a30a-e87f4d59c1ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n|dept_name|total_salary|\n+---------+------------+\n|    Stock|       10000|\n|       HR|       14000|\n|   Supply|        2000|\n+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col \n",
    "\n",
    "dfJoin = empDF.join(deptDF, empDF.employee_dept_id==deptDF.dept_id, \"inner\")\\\n",
    "                .withColumn(\"bonus\", col(\"salary\")*0.1)\\\n",
    "                .groupBy(\"dept_name\").agg(sum(\"salary\").alias(\"total_salary\"))\n",
    "                # .groupBy(\"dept_name\").sum(\"salary\") -- works too\n",
    "                \n",
    "dfJoin.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2323e2d9-61f6-4f18-b772-42bf973eca1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Physical plan is the default plan given by Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99bf6750-aa45-465d-9a4a-96f73e22070a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   *(4) HashAggregate(keys=[dept_name#246], functions=[finalmerge_sum(merge sum#868L) AS sum(salary#186L)#857L])\n   +- AQEShuffleRead coalesced\n      +- ShuffleQueryStage 2, Statistics(sizeInBytes=96.0 B, rowCount=3, isRuntime=true)\n         +- Exchange hashpartitioning(dept_name#246, 200), ENSURE_REQUIREMENTS, [plan_id=2402]\n            +- *(3) HashAggregate(keys=[dept_name#246], functions=[partial_sum(salary#186L) AS sum#868L])\n               +- *(3) Project [salary#186L, dept_name#246]\n                  +- *(3) SortMergeJoin [cast(employee_dept_id#184 as bigint)], [dept_id#247L], Inner\n                     :- Sort [cast(employee_dept_id#184 as bigint) ASC NULLS FIRST], false, 0\n                     :  +- AQEShuffleRead coalesced\n                     :     +- ShuffleQueryStage 0, Statistics(sizeInBytes=224.0 B, rowCount=7, isRuntime=true)\n                     :        +- Exchange hashpartitioning(cast(employee_dept_id#184 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=2286]\n                     :           +- *(1) Project [employee_dept_id#184, salary#186L]\n                     :              +- *(1) Filter isnotnull(employee_dept_id#184)\n                     :                 +- *(1) Scan ExistingRDD[employee_id#180L,first_name#181,last_name#182,doj#183,employee_dept_id#184,gender#185,salary#186L]\n                     +- Sort [dept_id#247L ASC NULLS FIRST], false, 0\n                        +- AQEShuffleRead coalesced\n                           +- ShuffleQueryStage 1, Statistics(sizeInBytes=128.0 B, rowCount=4, isRuntime=true)\n                              +- Exchange hashpartitioning(dept_id#247L, 200), ENSURE_REQUIREMENTS, [plan_id=2291]\n                                 +- *(2) Filter isnotnull(dept_id#247L)\n                                    +- *(2) Scan ExistingRDD[dept_name#246,dept_id#247L]\n+- == Initial Plan ==\n   HashAggregate(keys=[dept_name#246], functions=[finalmerge_sum(merge sum#868L) AS sum(salary#186L)#857L])\n   +- Exchange hashpartitioning(dept_name#246, 200), ENSURE_REQUIREMENTS, [plan_id=2198]\n      +- HashAggregate(keys=[dept_name#246], functions=[partial_sum(salary#186L) AS sum#868L])\n         +- Project [salary#186L, dept_name#246]\n            +- SortMergeJoin [cast(employee_dept_id#184 as bigint)], [dept_id#247L], Inner\n               :- Sort [cast(employee_dept_id#184 as bigint) ASC NULLS FIRST], false, 0\n               :  +- Exchange hashpartitioning(cast(employee_dept_id#184 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=2190]\n               :     +- Project [employee_dept_id#184, salary#186L]\n               :        +- Filter isnotnull(employee_dept_id#184)\n               :           +- Scan ExistingRDD[employee_id#180L,first_name#181,last_name#182,doj#183,employee_dept_id#184,gender#185,salary#186L]\n               +- Sort [dept_id#247L ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(dept_id#247L, 200), ENSURE_REQUIREMENTS, [plan_id=2191]\n                     +- Filter isnotnull(dept_id#247L)\n                        +- Scan ExistingRDD[dept_name#246,dept_id#247L]\n\n\n"
     ]
    }
   ],
   "source": [
    "dfJoin.explain() # dfJoin.explain(mode= \"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55b7b131-95ff-4787-8fc6-055851f88ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Aggregate ['dept_name], ['dept_name, sum('salary) AS total_salary#858]\n+- Project [employee_id#180L, first_name#181, last_name#182, doj#183, employee_dept_id#184, gender#185, salary#186L, dept_name#246, dept_id#247L, (cast(salary#186L as double) * 0.1) AS bonus#836]\n   +- Join Inner, (cast(employee_dept_id#184 as bigint) = dept_id#247L)\n      :- LogicalRDD [employee_id#180L, first_name#181, last_name#182, doj#183, employee_dept_id#184, gender#185, salary#186L], false\n      +- LogicalRDD [dept_name#246, dept_id#247L], false\n\n== Analyzed Logical Plan ==\ndept_name: string, total_salary: bigint\nAggregate [dept_name#246], [dept_name#246, sum(salary#186L) AS total_salary#858L]\n+- Project [employee_id#180L, first_name#181, last_name#182, doj#183, employee_dept_id#184, gender#185, salary#186L, dept_name#246, dept_id#247L, (cast(salary#186L as double) * 0.1) AS bonus#836]\n   +- Join Inner, (cast(employee_dept_id#184 as bigint) = dept_id#247L)\n      :- LogicalRDD [employee_id#180L, first_name#181, last_name#182, doj#183, employee_dept_id#184, gender#185, salary#186L], false\n      +- LogicalRDD [dept_name#246, dept_id#247L], false\n\n== Optimized Logical Plan ==\nAggregate [dept_name#246], [dept_name#246, sum(salary#186L) AS total_salary#858L]\n+- Project [salary#186L, dept_name#246]\n   +- Join Inner, (cast(employee_dept_id#184 as bigint) = dept_id#247L)\n      :- Project [employee_dept_id#184, salary#186L]\n      :  +- Filter isnotnull(employee_dept_id#184)\n      :     +- LogicalRDD [employee_id#180L, first_name#181, last_name#182, doj#183, employee_dept_id#184, gender#185, salary#186L], false\n      +- Filter isnotnull(dept_id#247L)\n         +- LogicalRDD [dept_name#246, dept_id#247L], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   *(4) HashAggregate(keys=[dept_name#246], functions=[finalmerge_sum(merge sum#868L) AS sum(salary#186L)#857L], output=[dept_name#246, total_salary#858L])\n   +- AQEShuffleRead coalesced\n      +- ShuffleQueryStage 2, Statistics(sizeInBytes=96.0 B, rowCount=3, isRuntime=true)\n         +- Exchange hashpartitioning(dept_name#246, 200), ENSURE_REQUIREMENTS, [plan_id=2402]\n            +- *(3) HashAggregate(keys=[dept_name#246], functions=[partial_sum(salary#186L) AS sum#868L], output=[dept_name#246, sum#868L])\n               +- *(3) Project [salary#186L, dept_name#246]\n                  +- *(3) SortMergeJoin [cast(employee_dept_id#184 as bigint)], [dept_id#247L], Inner\n                     :- Sort [cast(employee_dept_id#184 as bigint) ASC NULLS FIRST], false, 0\n                     :  +- AQEShuffleRead coalesced\n                     :     +- ShuffleQueryStage 0, Statistics(sizeInBytes=224.0 B, rowCount=7, isRuntime=true)\n                     :        +- Exchange hashpartitioning(cast(employee_dept_id#184 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=2286]\n                     :           +- *(1) Project [employee_dept_id#184, salary#186L]\n                     :              +- *(1) Filter isnotnull(employee_dept_id#184)\n                     :                 +- *(1) Scan ExistingRDD[employee_id#180L,first_name#181,last_name#182,doj#183,employee_dept_id#184,gender#185,salary#186L]\n                     +- Sort [dept_id#247L ASC NULLS FIRST], false, 0\n                        +- AQEShuffleRead coalesced\n                           +- ShuffleQueryStage 1, Statistics(sizeInBytes=128.0 B, rowCount=4, isRuntime=true)\n                              +- Exchange hashpartitioning(dept_id#247L, 200), ENSURE_REQUIREMENTS, [plan_id=2291]\n                                 +- *(2) Filter isnotnull(dept_id#247L)\n                                    +- *(2) Scan ExistingRDD[dept_name#246,dept_id#247L]\n+- == Initial Plan ==\n   HashAggregate(keys=[dept_name#246], functions=[finalmerge_sum(merge sum#868L) AS sum(salary#186L)#857L], output=[dept_name#246, total_salary#858L])\n   +- Exchange hashpartitioning(dept_name#246, 200), ENSURE_REQUIREMENTS, [plan_id=2198]\n      +- HashAggregate(keys=[dept_name#246], functions=[partial_sum(salary#186L) AS sum#868L], output=[dept_name#246, sum#868L])\n         +- Project [salary#186L, dept_name#246]\n            +- SortMergeJoin [cast(employee_dept_id#184 as bigint)], [dept_id#247L], Inner\n               :- Sort [cast(employee_dept_id#184 as bigint) ASC NULLS FIRST], false, 0\n               :  +- Exchange hashpartitioning(cast(employee_dept_id#184 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=2190]\n               :     +- Project [employee_dept_id#184, salary#186L]\n               :        +- Filter isnotnull(employee_dept_id#184)\n               :           +- Scan ExistingRDD[employee_id#180L,first_name#181,last_name#182,doj#183,employee_dept_id#184,gender#185,salary#186L]\n               +- Sort [dept_id#247L ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(dept_id#247L, 200), ENSURE_REQUIREMENTS, [plan_id=2191]\n                     +- Filter isnotnull(dept_id#247L)\n                        +- Scan ExistingRDD[dept_name#246,dept_id#247L]\n\n"
     ]
    }
   ],
   "source": [
    "dfJoin.explain(extended=True)  # dfJoin.explain(mode= \"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "423cfc14-959e-4692-b51d-9f38b790da39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Parsed Logical Plan (Unresolved Logical Plan) --> \n",
    "- Analyzed Logical Plan (Refers Schema Catalogue) --> \n",
    "- Optimized Logical Plan (Predicate Boost On) --> \n",
    "- Physical Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb6440ef-a3e7-4d4b-bf80-42465e2ba6b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan (30)\n+- == Final Plan ==\n   * HashAggregate (20)\n   +- AQEShuffleRead (19)\n      +- ShuffleQueryStage (18), Statistics(sizeInBytes=96.0 B, rowCount=3, isRuntime=true)\n         +- Exchange (17)\n            +- * HashAggregate (16)\n               +- * Project (15)\n                  +- * SortMergeJoin Inner (14)\n                     :- Sort (7)\n                     :  +- AQEShuffleRead (6)\n                     :     +- ShuffleQueryStage (5), Statistics(sizeInBytes=224.0 B, rowCount=7, isRuntime=true)\n                     :        +- Exchange (4)\n                     :           +- * Project (3)\n                     :              +- * Filter (2)\n                     :                 +- * Scan ExistingRDD (1)\n                     +- Sort (13)\n                        +- AQEShuffleRead (12)\n                           +- ShuffleQueryStage (11), Statistics(sizeInBytes=128.0 B, rowCount=4, isRuntime=true)\n                              +- Exchange (10)\n                                 +- * Filter (9)\n                                    +- * Scan ExistingRDD (8)\n+- == Initial Plan ==\n   HashAggregate (29)\n   +- Exchange (28)\n      +- HashAggregate (27)\n         +- Project (26)\n            +- SortMergeJoin Inner (25)\n               :- Sort (22)\n               :  +- Exchange (21)\n               :     +- Project (3)\n               :        +- Filter (2)\n               :           +- Scan ExistingRDD (1)\n               +- Sort (24)\n                  +- Exchange (23)\n                     +- Filter (9)\n                        +- Scan ExistingRDD (8)\n\n\n(1) Scan ExistingRDD [codegen id : 1]\nOutput [7]: [employee_id#180L, first_name#181, last_name#182, doj#183, employee_dept_id#184, gender#185, salary#186L]\nArguments: [employee_id#180L, first_name#181, last_name#182, doj#183, employee_dept_id#184, gender#185, salary#186L], MapPartitionsRDD[46] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n\n(2) Filter [codegen id : 1]\nInput [7]: [employee_id#180L, first_name#181, last_name#182, doj#183, employee_dept_id#184, gender#185, salary#186L]\nCondition : isnotnull(employee_dept_id#184)\n\n(3) Project [codegen id : 1]\nOutput [2]: [employee_dept_id#184, salary#186L]\nInput [7]: [employee_id#180L, first_name#181, last_name#182, doj#183, employee_dept_id#184, gender#185, salary#186L]\n\n(4) Exchange\nInput [2]: [employee_dept_id#184, salary#186L]\nArguments: hashpartitioning(cast(employee_dept_id#184 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=2825]\n\n(5) ShuffleQueryStage\nOutput [2]: [employee_dept_id#184, salary#186L]\nArguments: 0, Statistics(sizeInBytes=224.0 B, rowCount=7, isRuntime=true)\n\n(6) AQEShuffleRead\nInput [2]: [employee_dept_id#184, salary#186L]\nArguments: coalesced\n\n(7) Sort\nInput [2]: [employee_dept_id#184, salary#186L]\nArguments: [cast(employee_dept_id#184 as bigint) ASC NULLS FIRST], false, 0\n\n(8) Scan ExistingRDD [codegen id : 2]\nOutput [2]: [dept_name#246, dept_id#247L]\nArguments: [dept_name#246, dept_id#247L], MapPartitionsRDD[62] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n\n(9) Filter [codegen id : 2]\nInput [2]: [dept_name#246, dept_id#247L]\nCondition : isnotnull(dept_id#247L)\n\n(10) Exchange\nInput [2]: [dept_name#246, dept_id#247L]\nArguments: hashpartitioning(dept_id#247L, 200), ENSURE_REQUIREMENTS, [plan_id=2830]\n\n(11) ShuffleQueryStage\nOutput [2]: [dept_name#246, dept_id#247L]\nArguments: 1, Statistics(sizeInBytes=128.0 B, rowCount=4, isRuntime=true)\n\n(12) AQEShuffleRead\nInput [2]: [dept_name#246, dept_id#247L]\nArguments: coalesced\n\n(13) Sort\nInput [2]: [dept_name#246, dept_id#247L]\nArguments: [dept_id#247L ASC NULLS FIRST], false, 0\n\n(14) SortMergeJoin [codegen id : 3]\nLeft keys [1]: [cast(employee_dept_id#184 as bigint)]\nRight keys [1]: [dept_id#247L]\nJoin type: Inner\nJoin condition: None\n\n(15) Project [codegen id : 3]\nOutput [2]: [salary#186L, dept_name#246]\nInput [4]: [employee_dept_id#184, salary#186L, dept_name#246, dept_id#247L]\n\n(16) HashAggregate [codegen id : 3]\nInput [2]: [salary#186L, dept_name#246]\nKeys [1]: [dept_name#246]\nFunctions [1]: [partial_sum(salary#186L) AS sum#980L]\nAggregate Attributes [1]: [sum#979L]\nResults [2]: [dept_name#246, sum#980L]\n\n(17) Exchange\nInput [2]: [dept_name#246, sum#980L]\nArguments: hashpartitioning(dept_name#246, 200), ENSURE_REQUIREMENTS, [plan_id=2941]\n\n(18) ShuffleQueryStage\nOutput [2]: [dept_name#246, sum#980L]\nArguments: 2, Statistics(sizeInBytes=96.0 B, rowCount=3, isRuntime=true)\n\n(19) AQEShuffleRead\nInput [2]: [dept_name#246, sum#980L]\nArguments: coalesced\n\n(20) HashAggregate [codegen id : 4]\nInput [2]: [dept_name#246, sum#980L]\nKeys [1]: [dept_name#246]\nFunctions [1]: [finalmerge_sum(merge sum#980L) AS sum(salary#186L)#969L]\nAggregate Attributes [1]: [sum(salary#186L)#969L]\nResults [2]: [dept_name#246, sum(salary#186L)#969L AS total_salary#970L]\n\n(21) Exchange\nInput [2]: [employee_dept_id#184, salary#186L]\nArguments: hashpartitioning(cast(employee_dept_id#184 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=2729]\n\n(22) Sort\nInput [2]: [employee_dept_id#184, salary#186L]\nArguments: [cast(employee_dept_id#184 as bigint) ASC NULLS FIRST], false, 0\n\n(23) Exchange\nInput [2]: [dept_name#246, dept_id#247L]\nArguments: hashpartitioning(dept_id#247L, 200), ENSURE_REQUIREMENTS, [plan_id=2730]\n\n(24) Sort\nInput [2]: [dept_name#246, dept_id#247L]\nArguments: [dept_id#247L ASC NULLS FIRST], false, 0\n\n(25) SortMergeJoin\nLeft keys [1]: [cast(employee_dept_id#184 as bigint)]\nRight keys [1]: [dept_id#247L]\nJoin type: Inner\nJoin condition: None\n\n(26) Project\nOutput [2]: [salary#186L, dept_name#246]\nInput [4]: [employee_dept_id#184, salary#186L, dept_name#246, dept_id#247L]\n\n(27) HashAggregate\nInput [2]: [salary#186L, dept_name#246]\nKeys [1]: [dept_name#246]\nFunctions [1]: [partial_sum(salary#186L) AS sum#980L]\nAggregate Attributes [1]: [sum#979L]\nResults [2]: [dept_name#246, sum#980L]\n\n(28) Exchange\nInput [2]: [dept_name#246, sum#980L]\nArguments: hashpartitioning(dept_name#246, 200), ENSURE_REQUIREMENTS, [plan_id=2737]\n\n(29) HashAggregate\nInput [2]: [dept_name#246, sum#980L]\nKeys [1]: [dept_name#246]\nFunctions [1]: [finalmerge_sum(merge sum#980L) AS sum(salary#186L)#969L]\nAggregate Attributes [1]: [sum(salary#186L)#969L]\nResults [2]: [dept_name#246, sum(salary#186L)#969L AS total_salary#970L]\n\n(30) AdaptiveSparkPlan\nOutput [2]: [dept_name#246, total_salary#970L]\nArguments: isFinalPlan=true\n\n\n"
     ]
    }
   ],
   "source": [
    "dfJoin.explain(mode= \"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf755431-19cc-4b9b-a9e9-30a335931b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\nAggregate [dept_name#246], [dept_name#246, sum(salary#186L) AS total_salary#970L], Statistics(sizeInBytes=6.86E+35 B)\n+- Project [salary#186L, dept_name#246], Statistics(sizeInBytes=6.86E+35 B)\n   +- Join Inner, (cast(employee_dept_id#184 as bigint) = dept_id#247L), Statistics(sizeInBytes=1.22E+36 B)\n      :- Project [employee_dept_id#184, salary#186L], Statistics(sizeInBytes=2.3 EiB)\n      :  +- Filter isnotnull(employee_dept_id#184), Statistics(sizeInBytes=8.0 EiB)\n      :     +- LogicalRDD [employee_id#180L, first_name#181, last_name#182, doj#183, employee_dept_id#184, gender#185, salary#186L], false, Statistics(sizeInBytes=8.0 EiB)\n      +- Filter isnotnull(dept_id#247L), Statistics(sizeInBytes=8.0 EiB)\n         +- LogicalRDD [dept_name#246, dept_id#247L], false, Statistics(sizeInBytes=8.0 EiB)\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   *(4) HashAggregate(keys=[dept_name#246], functions=[finalmerge_sum(merge sum#980L) AS sum(salary#186L)#969L], output=[dept_name#246, total_salary#970L])\n   +- AQEShuffleRead coalesced\n      +- ShuffleQueryStage 2, Statistics(sizeInBytes=96.0 B, rowCount=3, isRuntime=true)\n         +- Exchange hashpartitioning(dept_name#246, 200), ENSURE_REQUIREMENTS, [plan_id=2941]\n            +- *(3) HashAggregate(keys=[dept_name#246], functions=[partial_sum(salary#186L) AS sum#980L], output=[dept_name#246, sum#980L])\n               +- *(3) Project [salary#186L, dept_name#246]\n                  +- *(3) SortMergeJoin [cast(employee_dept_id#184 as bigint)], [dept_id#247L], Inner\n                     :- Sort [cast(employee_dept_id#184 as bigint) ASC NULLS FIRST], false, 0\n                     :  +- AQEShuffleRead coalesced\n                     :     +- ShuffleQueryStage 0, Statistics(sizeInBytes=224.0 B, rowCount=7, isRuntime=true)\n                     :        +- Exchange hashpartitioning(cast(employee_dept_id#184 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=2825]\n                     :           +- *(1) Project [employee_dept_id#184, salary#186L]\n                     :              +- *(1) Filter isnotnull(employee_dept_id#184)\n                     :                 +- *(1) Scan ExistingRDD[employee_id#180L,first_name#181,last_name#182,doj#183,employee_dept_id#184,gender#185,salary#186L]\n                     +- Sort [dept_id#247L ASC NULLS FIRST], false, 0\n                        +- AQEShuffleRead coalesced\n                           +- ShuffleQueryStage 1, Statistics(sizeInBytes=128.0 B, rowCount=4, isRuntime=true)\n                              +- Exchange hashpartitioning(dept_id#247L, 200), ENSURE_REQUIREMENTS, [plan_id=2830]\n                                 +- *(2) Filter isnotnull(dept_id#247L)\n                                    +- *(2) Scan ExistingRDD[dept_name#246,dept_id#247L]\n+- == Initial Plan ==\n   HashAggregate(keys=[dept_name#246], functions=[finalmerge_sum(merge sum#980L) AS sum(salary#186L)#969L], output=[dept_name#246, total_salary#970L])\n   +- Exchange hashpartitioning(dept_name#246, 200), ENSURE_REQUIREMENTS, [plan_id=2737]\n      +- HashAggregate(keys=[dept_name#246], functions=[partial_sum(salary#186L) AS sum#980L], output=[dept_name#246, sum#980L])\n         +- Project [salary#186L, dept_name#246]\n            +- SortMergeJoin [cast(employee_dept_id#184 as bigint)], [dept_id#247L], Inner\n               :- Sort [cast(employee_dept_id#184 as bigint) ASC NULLS FIRST], false, 0\n               :  +- Exchange hashpartitioning(cast(employee_dept_id#184 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=2729]\n               :     +- Project [employee_dept_id#184, salary#186L]\n               :        +- Filter isnotnull(employee_dept_id#184)\n               :           +- Scan ExistingRDD[employee_id#180L,first_name#181,last_name#182,doj#183,employee_dept_id#184,gender#185,salary#186L]\n               +- Sort [dept_id#247L ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(dept_id#247L, 200), ENSURE_REQUIREMENTS, [plan_id=2730]\n                     +- Filter isnotnull(dept_id#247L)\n                        +- Scan ExistingRDD[dept_name#246,dept_id#247L]\n\n\n"
     ]
    }
   ],
   "source": [
    "dfJoin.explain(mode= \"cost\") # Shows only optimized and physical plan"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "47_Explain_Plan",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}