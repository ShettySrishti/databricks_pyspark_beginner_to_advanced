{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49232d04-9446-4b23-9838-cdff37fe7b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a Sample Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a9c3131-3a4f-46b8-8f24-e16a5a909bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+\n|store_id|  item|amount|\n+--------+------+------+\n|     101| apple|   5.5|\n|     101|banana|  2.75|\n|     102|  milk|   3.2|\n|     103| bread|   2.0|\n|     102|  eggs|   4.1|\n|     104|cheese|   6.3|\n|     105| juice|   3.8|\n|     101|butter|   2.9|\n|     103|yogurt|   1.5|\n|     104|cereal|  4.25|\n+--------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transactions = [\n",
    "    (101, \"apple\", 5.50),\n",
    "    (101, \"banana\", 2.75),\n",
    "    (102, \"milk\", 3.20),\n",
    "    (103, \"bread\", 2.00),\n",
    "    (102, \"eggs\", 4.10),\n",
    "    (104, \"cheese\", 6.30),\n",
    "    (105, \"juice\", 3.80),\n",
    "    (101, \"butter\", 2.90),\n",
    "    (103, \"yogurt\", 1.50),\n",
    "    (104, \"cereal\", 4.25)\n",
    "]\n",
    "columns = [\"store_id\", \"item\", \"amount\"]\n",
    "\n",
    "transactionsDF = spark.createDataFrame(transactions, columns)\n",
    "transactionsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b868051f-db43-44f2-8c4d-3b0ea8004ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Sample Dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60ad308c-53aa-4e41-948b-738fcfec9e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n|store_id|        store_name|\n+--------+------------------+\n|     101|   Downtown Market|\n|     102|    Uptown Grocers|\n|     103|      Central Mart|\n|     104|       Fresh Foods|\n|     105|Neighborhood Store|\n+--------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "store = [\n",
    "    (101, \"Downtown Market\"),\n",
    "    (102, \"Uptown Grocers\"),\n",
    "    (103, \"Central Mart\"),\n",
    "    (104, \"Fresh Foods\"),\n",
    "    (105, \"Neighborhood Store\")\n",
    "]\n",
    "store_columns = [\"store_id\", \"store_name\"]\n",
    "\n",
    "storeDF = spark.createDataFrame(store, store_columns)\n",
    "storeDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57d7346b-6925-41f5-b38f-0af1a15b94ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### When AQE is Enabled (True by default in Spark 3.0+ versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be0d4ba-8f76-4db8-b612-15e1bf0ee32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+--------+------------------+\n|store_id|  item|amount|store_id|        store_name|\n+--------+------+------+--------+------------------+\n|     101| apple|   5.5|     101|   Downtown Market|\n|     101|banana|  2.75|     101|   Downtown Market|\n|     102|  milk|   3.2|     102|    Uptown Grocers|\n|     103| bread|   2.0|     103|      Central Mart|\n|     102|  eggs|   4.1|     102|    Uptown Grocers|\n|     104|cheese|   6.3|     104|       Fresh Foods|\n|     105| juice|   3.8|     105|Neighborhood Store|\n|     101|butter|   2.9|     101|   Downtown Market|\n|     103|yogurt|   1.5|     103|      Central Mart|\n|     104|cereal|  4.25|     104|       Fresh Foods|\n+--------+------+------+--------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joinDF = transactionsDF.join(broadcast(storeDF), transactionsDF['store_id'] == storeDF['store_id'])\n",
    "joinDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45c9a6d9-ff2c-4ea7-b67c-5c3fa52fae83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- No Shuffle of Large Table involved in Broadcast Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0647308c-469f-46cb-b856-f0185c7320ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- BroadcastHashJoin [store_id#296L], [store_id#315L], Inner, BuildRight, false, true\n   :- Filter isnotnull(store_id#296L)\n   :  +- Scan ExistingRDD[store_id#296L,item#297,amount#298]\n   +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=2513]\n      +- Filter isnotnull(store_id#315L)\n         +- Scan ExistingRDD[store_id#315L,store_name#316]\n\n\n"
     ]
    }
   ],
   "source": [
    "joinDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24926e20-2fb9-42f3-9cca-25648aa817d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\nJoin Inner, (store_id#296L = store_id#315L)\n:- LogicalRDD [store_id#296L, item#297, amount#298], false\n+- ResolvedHint (strategy=broadcast)\n   +- LogicalRDD [store_id#315L, store_name#316], false\n\n== Analyzed Logical Plan ==\nstore_id: bigint, item: string, amount: double, store_id: bigint, store_name: string\nJoin Inner, (store_id#296L = store_id#315L)\n:- LogicalRDD [store_id#296L, item#297, amount#298], false\n+- ResolvedHint (strategy=broadcast)\n   +- LogicalRDD [store_id#315L, store_name#316], false\n\n== Optimized Logical Plan ==\nJoin Inner, (store_id#296L = store_id#315L), rightHint=(strategy=broadcast)\n:- Filter isnotnull(store_id#296L)\n:  +- LogicalRDD [store_id#296L, item#297, amount#298], false\n+- Filter isnotnull(store_id#315L)\n   +- LogicalRDD [store_id#315L, store_name#316], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- BroadcastHashJoin [store_id#296L], [store_id#315L], Inner, BuildRight, false, true\n   :- Filter isnotnull(store_id#296L)\n   :  +- Scan ExistingRDD[store_id#296L,item#297,amount#298]\n   +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=1841]\n      +- Filter isnotnull(store_id#315L)\n         +- Scan ExistingRDD[store_id#315L,store_name#316]\n\n"
     ]
    }
   ],
   "source": [
    "joinDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84ef09ff-450d-4fe2-ac94-70bb43fd87a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### When AQE is disabled (False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7344d1-af56-4f24-9a83-95adb1772aa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[40]: 'false'"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.get(\"spark.sql.adaptive.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b3fb16d-5264-42c0-bfeb-0a181969dfdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+--------+------------------+\n|store_id|  item|amount|store_id|        store_name|\n+--------+------+------+--------+------------------+\n|     101| apple|   5.5|     101|   Downtown Market|\n|     101|banana|  2.75|     101|   Downtown Market|\n|     102|  milk|   3.2|     102|    Uptown Grocers|\n|     103| bread|   2.0|     103|      Central Mart|\n|     102|  eggs|   4.1|     102|    Uptown Grocers|\n|     104|cheese|   6.3|     104|       Fresh Foods|\n|     105| juice|   3.8|     105|Neighborhood Store|\n|     101|butter|   2.9|     101|   Downtown Market|\n|     103|yogurt|   1.5|     103|      Central Mart|\n|     104|cereal|  4.25|     104|       Fresh Foods|\n+--------+------+------+--------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joinDF1 = transactionsDF.join(broadcast(storeDF), transactionsDF['store_id'] == storeDF['store_id'])\n",
    "joinDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2504815d-36bf-40e8-a9ec-b468dd20a5de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(2) BroadcastHashJoin [store_id#296L], [store_id#315L], Inner, BuildRight, false, false\n:- *(2) Filter isnotnull(store_id#296L)\n:  +- *(2) Scan ExistingRDD[store_id#296L,item#297,amount#298]\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=2666]\n   +- *(1) Filter isnotnull(store_id#315L)\n      +- *(1) Scan ExistingRDD[store_id#315L,store_name#316]\n\n\n"
     ]
    }
   ],
   "source": [
    "joinDF1.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6f7e701-a70f-4c59-959e-fb48bd5c9fb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Summary:**\n",
    "\n",
    "- AQE (adaptive.enabled=True) allows Spark to optimize the plan during execution, so the explain output is more dynamic and may show adaptive nodes.\n",
    "- With AQE off, the plan is static and shows the exact operators Spark will use, making the explain output more straightforward and final."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "16_Broadcast_Variable",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}