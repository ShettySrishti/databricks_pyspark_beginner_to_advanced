{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cc24ad7-ed96-487f-b810-549804b9f3bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Default Partitions for RDD/DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3c36412-4c24-4d2e-b15d-fe706b6f7a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`sc.defaultParallelism`\n",
    "\n",
    "- Sets the default number of partitions for RDD operations and shuffles.\n",
    "- Usually equals the total number of executor cores in the Spark cluster.\n",
    "- Creates 8 paritions by default\n",
    "\n",
    "`spark.sql.files.maxPartitionBytes`\n",
    "\n",
    "- Sets the max data size (in bytes) per partition when reading files (default: 128 MB).\n",
    "- Lowering increases parallelism (more partitions); raising reduces parallelism (fewer partitions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4a5c8d1-26d7-470e-b178-c8c9d78d22f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Check Default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d48dd9-6831-41b8-99ab-7d432c814853",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: 8"
     ]
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ecaff8-ece6-4942-ba34-e8ddcf1cb5a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: '134217728b'"
     ]
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.files.maxPartitionBytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1138ac5b-d9da-4f67-8fa1-6fa9ccc5e4d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate data with Spark Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0002647b-cba2-45e0-afea-1274a6667dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: 8"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df = spark.createDataFrame(range(10), IntegerType())\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e328558-4d3d-4acc-a095-1bbd3e26d5fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verify the data with all Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c2428f-29bc-4482-8a02-d34af2ac36c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: [[Row(value=0)],\n [Row(value=1)],\n [Row(value=2)],\n [Row(value=3), Row(value=4)],\n [Row(value=5)],\n [Row(value=6)],\n [Row(value=7)],\n [Row(value=8), Row(value=9)]]"
     ]
    }
   ],
   "source": [
    "df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb415490-ced8-444b-b5eb-01f39a0d2f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read External File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656c5a26-3238-4bc1-a1db-f29de11f2433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: [FileInfo(path='dbfs:/FileStore/dbutils_test/superstore.csv', name='superstore.csv', size=229150, modificationTime=1759061573000),\n FileInfo(path='dbfs:/FileStore/dbutils_test/superstore_sample.csv', name='superstore_sample.csv', size=210298, modificationTime=1759061578000),\n FileInfo(path='dbfs:/FileStore/dbutils_test/test.text', name='test.text', size=33, modificationTime=1759061711000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/FileStore/dbutils_test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e36242-7c4b-434a-92f0-bb8bf7d21c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: 3"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", \";\").csv(\"/FileStore/dbutils_test/\")\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4a4f2a9-f1d7-463c-a94b-c595529aaae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Change the `maxPartitionBytes` parameter which changes the Number of Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a63219e-d290-4911-b24d-ab3c353bbc7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[29]: '200000'"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"200000\")\n",
    "spark.conf.get(\"spark.sql.files.maxPartitionBytes\") # verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bff7268a-2f97-47c1-a335-1124d48aa8af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: 5"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", \";\").csv(\"/FileStore/dbutils_test/\")\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9634a7c9-392a-4fde-a749-a23f944463ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating Single Partition with all data is not good for performance, as one core would process entire data while all other cores are kept idle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566f09fa-9286-42e0-9a31-5dd63ae42047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[31]: 1"
     ]
    }
   ],
   "source": [
    "rdd2 = sc.parallelize(range(100), 1)\n",
    "rdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb1306d5-0d6a-46e6-abdd-99797c36c9c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f40a9ca-c19c-45d7-af28-29d79ccb5de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: [[Row(value=0)],\n [Row(value=1)],\n [Row(value=2)],\n [Row(value=3), Row(value=4)],\n [Row(value=5)],\n [Row(value=6)],\n [Row(value=7)],\n [Row(value=8), Row(value=9)]]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df = spark.createDataFrame(range(10), IntegerType())\n",
    "df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e6f079e-d113-4852-ab9c-874e559b8b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[33]: 20"
     ]
    }
   ],
   "source": [
    "df1 = df.repartition(20)\n",
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1df9711c-2280-495b-b52a-01104a73a57c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[35]: [[],\n [Row(value=8)],\n [Row(value=9)],\n [],\n [Row(value=1)],\n [],\n [Row(value=6)],\n [],\n [Row(value=3)],\n [Row(value=0), Row(value=2), Row(value=4)],\n [],\n [Row(value=7)],\n [],\n [],\n [Row(value=5)],\n [],\n [],\n [],\n [],\n []]"
     ]
    }
   ],
   "source": [
    "df1.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c496b42-8d58-4e90-9740-e4bc4cccdb17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[38]: 2"
     ]
    }
   ],
   "source": [
    "df1 = df.repartition(2)\n",
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5572c32f-4bfe-4b34-a696-dd0205498665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[39]: [[Row(value=2),\n  Row(value=3),\n  Row(value=5),\n  Row(value=6),\n  Row(value=7),\n  Row(value=9)],\n [Row(value=0), Row(value=1), Row(value=4), Row(value=8)]]"
     ]
    }
   ],
   "source": [
    "df1.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f0e5866-5868-4fad-a63a-2f8d03929b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a5d828-c7c6-487d-b22e-0418a389413a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[40]: 2"
     ]
    }
   ],
   "source": [
    "df2 = df.coalesce(2)\n",
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2035adbf-35f2-427f-8e82-74ebc23f4c01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[41]: [[Row(value=0), Row(value=1), Row(value=2), Row(value=3), Row(value=4)],\n [Row(value=5), Row(value=6), Row(value=7), Row(value=8), Row(value=9)]]"
     ]
    }
   ],
   "source": [
    "df2.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6182b2b9-f645-4c20-b02d-048328444a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`coalesce`\n",
    "\n",
    "- Reduces the number of partitions by merging existing ones without a full shuffle (faster, less expensive).\n",
    "- Often used to decrease partitions before writing data (e.g., to create a single output file).\n",
    "- Does not guarantee even data distribution; some partitions may be much larger than others.\n",
    "\n",
    "`repartition`\n",
    "\n",
    "- Changes the number of partitions by performing a full shuffle of the data (more expensive).\n",
    "- Can both increase or decrease the number of partitions and ensures even data distribution.\n",
    "- Useful for optimizing parallelism before expensive operations or balancing data across partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b86aa497-54e2-4855-adf8-78df67ad219b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba2a3c30-9b82-4aa0-b5a9-f564623cf7ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "15_Repartition_and_Coalesce",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}